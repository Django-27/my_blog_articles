---
title: 技术杂项_1_记录(已完成)
date: 2017-03-30
tags:
---

# 1 启动一个简单的服务器
    Python2 python -m SimpleHTTPServer 8080
    Python3 python -m http.serve
# 2 正则表达式匹配代理
    r = re.compile("[0-9]+(?:\.[0-9]+){3}:\d{2,4}")
    r.findall('123.45.56.78:80')
    ['123.45.56.78:80']
# 3 (?:pattern)
匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 "或" 字符 (|) 来组合一个模式的各个部分是很有用。例如， 'industr(?:y|ies) 就是一个比 'industry|industries' 更简略的表达式。那么(?:13\d|15\d)也就是要在这里面选择匹配13+0~9的数字一个或15+0~9的数字
# 4 \\*
因为\*是正则表达式里的特殊字符，所以，这里要匹配字符串里的*时，得用转义字符“\”
那么\\\*{3}就是要匹配“****”
# 5 windows关机命令
    shutdown -s -f # 关机+强制
    shutdown -a    # 取消
    shutdown -s -t 5
# 6 list Comprehensions 列表推导式 和 生成器表达式
这是一种将for if 赋值放到单一语句中的一种方法，即通过一个表达式对一个列表做映射或过滤操作

    nums = [1, 4, -1, 7, 9]
    filterd_and_squared = [x**2 for x in nums if x > 0]

负面效应：一次加载到内存中；可以使用生成器解决，有着几乎相同的语法，但生成器表达式被圆括号包围

    filter_and_squared = (x**2 for x in nums if x > 0)
    for item in filter_and_squared:
        print(item)
    print(list(filter_and_squared)

   
# 7 构建路由规则

一个 web 应用不同的路径会有不同的处理函数，路由就是根据请求的 URL 找到对应处理函数的过程; flask中有两种方法（还有一种非常规方法app.url_map）：

 1. @app.route() 生成器

    @app.route('/')
    def hello():
        return "hello world"
 2. app.add_url_rule 方法签名，包括URL规则字符串和对于URL的处理函数

    def hello():
        return "hello world"
    app.add_url_rule('/', 'hello', hello)

事实上，flask核心的路由逻辑在werkzeug中实现，实现URL到endpoint的转换；而endpoint和view function之间的匹配关系flask存放到了字典中

# 8 短连接bit.ly不能访问
通过在chrome中安装插件Tampermonkey可以解决；如http://bit.ly/28UUgW3 ；安装方式是通过在插件界面新建文本，保存js代码，之后便实现了安装
插件代码选择由Greasy Fork下载![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/1DB15B00A761468F8245233BFE906285/1360)

# 10 字符类型检测
以前是chardet,现在是cchardet,提供更快的检测,安装cchardet包含了chardet

    import cchardet as chardet 
    with open('tmp.txt', 'rb') as f:
        print(chardet.detect(f.read))
    >> {'confidence': **, 'encoding': **}
    
    print(charset.detect(requests.get('http://www.baidu.com').text))
    >> ValueError: Expected a bytes object, not a unicode object
    print(charset.detect(requests.get('http://www.baidu.com').content))
    >> {'encoding': 'utf-8', 'confidence': 0.99}
    
    '¼à¿ØÖÐÐÄ'.encode('latin-1').decode('gbk')
    >> '监控中心'

# 11 笔记由小书匠迁移到有道云（图片问题）
小书匠的问题是不稳定，有时都不能编辑，侧边栏在编辑的时候也不跟着走
有道云在markdown中添加本地图片比较麻烦
- 先将本地图片上传，分享，打开后右键找到图片的地址，插入时使用这个图片地址即可
- 另一种方法是新建一个笔记，这样所有的图片都可以放进去，也是分享，打开分享链接，找对对应的图片，右键复制就得到了图片的地址
- speed_27  speed1478
 

# 12 布隆滤波器（Bloom Filter）
- 由波顿·布隆与1970年提出，只需要哈希表1/8到1/4的大小就可解决；它实际上是一个很长的**二进制向量**和一系列**随机映射函数**。例如：存储16亿个电子邮件的地址，每个地址用16位表示（可能8位既可以表示一个电子邮箱的信息指纹，但考虑哈希的效率为50%，使用16位），将是16亿二进制（比特），即2亿字节的向量，然后将这**16亿**二进制位全部清零；对于每一个电子邮件地址**X**,用8个不同的随机数产生器（F1，F2，··，F8）产生8个信息指纹(f1,f2,··，f8)，再使用一个随机数产生器**G**把这8个信息指纹映射到1-16亿中的8个自然数g1,g2,··,g8，**最后**将这8个位置的二进制全部设置为1。对1亿个点至邮件地址都这样处理后，一个针对这些电子邮件地址的布隆滤波器就建好了。
- 同理，检测一个可以的电子邮件地址是否在黑名单中，即执行上面相同的操作后看对应的8个二进制位是不是都是1.
- 布隆滤波器决不会漏掉黑名单中的任何一个可以地址，但有可能将一个不在黑名单中的误判，上面的这个例子中误识别率在万分之一以下。好处是快速、省空间，但有一定的误识别率，常见的补救方法是再建一个小的白名单，存储那些可能被误判的邮件地址。
- 有关误识别率统计（假阳性false positive rate）[曹培](http://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.html) m表示滤波器有m比特，里面有n个元素（插入n个元素后），每个元素对应的是k个信息指纹的哈希函数。Here is [a very nice visualization of how it works](https://www.jasondavies.com/bloomfilter/)![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/7DD4E0C67D854D6480E497A536AA2F7A/1256)


- pip install cython
- pip install pybloomfiltermmap3

```
>>> import pybloomfilter
>>> fruit = pybloomfilter.BloomFilter(100000, 0.1, '/tmp/words.bloom')
>>> fruit.update(('apple', 'pear', 'orange', 'apple'))
>>> len(fruit)
3
>>> 'mike' in fruit
False
>>> 'apple' in fruit
True
```
(pybloom module) |  (this module)
-|-
pybloom load took 0.76436 s/run | pybloomfilter load took 0.05423 s/run
pybloom tests took 0.16205 s/run | pybloomfilter tests took 0.00659 s/run
Errors: 0.25% positive 0.00% negative | Errors: 0.26% positive 0.00% negative


# 13 基于Redistribute的Bloomfilter URL去重
- 数据量不大时可以直接放在内存中去重，如Python的set()进行；当去重数据需要持久化时可以使用redistribute的set数据结构；当数据量再大时可以使用不同的加密算法先将字符串压缩成16/32/40个字符，然后使用上面的两种方法；
- 当数据量达到亿数量级时，内存有限必须使用‘位’来去重，才能满足需求；然而Bloomfilter运行在一台机器上的内存上，不方便持久化，机器down掉就什么也没有了，也不方便分布式爬虫的统一去重，可以在redis上申请内存进行Bloomfilter，即可解决上面的问题。(redis总string一个最大为512M，在多的数据就要分块了) 
```
# encoding=utf-8              # Py2

import redis
from hashlib import md5


class SimpleHash(object):
    def __init__(self, cap, seed):
        self.cap = cap
        self.seed = seed

    def hash(self, value):
        ret = 0
        for i in range(len(value)):
            ret += self.seed * ret + ord(value[i])
        return (self.cap - 1) & ret


class BloomFilter(object):
    def __init__(self, host='localhost', port=6379, db=0, blockNum=1, key='bloomfilter'):
        """
        :param host: the host of Redis
        :param port: the port of Redis
        :param db: witch db in Redis
        :param blockNum: one blockNum for about 90,000,000; if you have more strings for filtering, increase it.
        :param key: the key's name in Redis
        """
        self.server = redis.Redis(host=host, port=port, db=db)
        self.bit_size = 1 << 31  # Redis的String类型最大容量为512M，现使用256M
        self.seeds = [5, 7, 11, 13, 31, 37, 61]
        self.key = key
        self.blockNum = blockNum
        self.hashfunc = []
        for seed in self.seeds:
            self.hashfunc.append(SimpleHash(self.bit_size, seed))

    def isContains(self, str_input):
        if not str_input:
            return False
        m5 = md5()
        m5.update(str_input)
        str_input = m5.hexdigest()
        ret = True
        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)
        for f in self.hashfunc:
            loc = f.hash(str_input)
            ret = ret & self.server.getbit(name, loc)
        return ret

    def insert(self, str_input):
        m5 = md5()
        m5.update(str_input)
        str_input = m5.hexdigest()
        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)
        for f in self.hashfunc:
            loc = f.hash(str_input)
            self.server.setbit(name, loc, 1)


if __name__ == '__main__':
""" 第一次运行时会显示 not exists!，之后再运行会显示 exists! """
    bf = BloomFilter()
    if bf.isContains('http://www.baidu.com'):   # 判断字符串是否存在
        print 'exists!'
    else:
        print 'not exists!'
        bf.insert('http://www.baidu.com')    
```


# 14 scrapy_redis去重优化                  
- scrapy默认是开启了去重的，scrapy_redis会用自己的scheduler替代scrapy框架的scheduler进行任务调度；在scrpay_redis模块下查看scheduler.py源码。
- •在open()方法中有句self.df = RFPDupeFilter(...)，可见去重应该是用了RFPDupeFilter这个类；再看下面的enqueue_request()方法，里面有句if not request.dont_filter and self.df.request_seen(request):return，看来self.df.request_seen()这就是用来去重的了。
- •按住Ctrl再左键点击request_seen查看它的代码，可看到下面的代码：
```
def request_seen(self, request):
    fp = request_fingerprint(request)
    added = self.server.sadd(self.key, fp)
    return not added1
```
- 是否可用Bloomfilter进行优化？ 
 以上步骤可以看出，我们只要在这个request_seen()方法上面动些手脚即可。由于现有的七亿多去重数据存的都是这个fingerprint，所有Bloomfilter去重的对象仍然是request对象的fingerprint。更改后的代码如下：
```
def request_seen(self, request):
    fp = request_fingerprint(request)
    if self.bf.isContains(fp):  # 如果已经存在
        return True
    else:
        self.bf.insert(fp) # self.bf是类Bloomfilter()的实例化
        return False1 
```
# 15 优化的思路和代码就是这样；以下将已有的七亿多的去重数据转成Bloomfilter去重
- 内存将爆，动作稍微大点机器就能死掉，更别说Bloomfilter在上面申请内存了。当务之急肯定是将那七亿多个fingerprint导出到硬盘上，而且不能用本机导，并且先要将redis的自动持久化给关掉。
- 因为常用Mongo，所以习惯性首先想到Mongodb，从redis取出2000条再一次性插入Mongo，但速度还是不乐观，瓶颈在于MongoDB。（猜测是MongoDB对_id的去重导致的，也可能是物理硬件的限制）
- 后来想用SSDB，因为SSDB和Redis很相似，用list存肯定速度快很多。然而SSDB唯独不支持Centos7，其他版本的系统都可。。
- 最后才想起来用txt，这个最傻的方法，却是非常有效的方法。速度很快，只是为了防止读取时内存不足，每100万个fingerprint存在了一个txt，四台机器txt总共有七百个左右。
- fingerprint取出来后redis只剩下一千多万的Request种子，占用内存9G+。然后用Bloomfilter将txt中的fingerprint写回Redis，写完以后Redis占用内存25G，开启redis自动持久化后内存占用49G左右。
- 如果待去重的数据量比较大，需要修改scrapy_redis/dupefilter.py中第14行的blockNum值，默认blockNum=1。Bloomfilter算法是有漏失概率的（即不存在的会误判为存在），在保证漏失率小于万分之一的情况下，一个blockNum可满足7千万条数据的去重，一个blockNum占用256M内存（注意Linux如果开了自动持久化，redis占用内存会加倍）。

```
from scrapy import Spider, signals
from scrapy.exceptions import DontCloseSpider

from . import connection

class RedisMixin(object):
    """Mixin class to implement reading urls from a redis queue."""
    redis_key = None  # use default '<spider>:start_urls'

    def setup_redis(self):
        """Setup redis connection and idle signal.

        This should be called after the spider has set its crawler object.
        """
        if not self.redis_key:
            self.redis_key = '%s:start_urls' % self.name

        self.server = connection.from_settings(self.crawler.settings)
        # idle signal is called when the spider has no requests left,
        # that's when we will schedule new requests from redis queue
        self.crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)
        self.crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)
        self.log("Reading URLs from redis list '%s'" % self.redis_key)

    def next_request(self):
        """Returns a request to be scheduled or none."""
        url = self.server.lpop(self.redis_key)
        if url:
            return self.make_requests_from_url(url)

    def schedule_next_request(self):
        """Schedules a request if available"""
        req = self.next_request()
        if req:
            self.crawler.engine.crawl(req, spider=self)

    def spider_idle(self):
        """Schedules a request if available, otherwise waits."""
        self.schedule_next_request()
        raise DontCloseSpider

    def item_scraped(self, *args, **kwargs):
        """Avoids waiting for the spider to  idle before scheduling the next request"""
        self.schedule_next_request()


class RedisSpider(RedisMixin, Spider):
    """Spider that reads urls from redis queue when idle."""

    def _set_crawler(self, crawler):
        super(RedisSpider, self)._set_crawler(crawler)
        self.setup_redis()

```

```

from scrapyWithBloomfilter_demo.scrapy_redis.spiders import RedisSpider
from scrapy.http import Request


class Spider1(RedisSpider):
    name = "Spider1"
    redis_key = "scrapyWithBloomfilter_demo:start_urls"
    start_urls = [
        'http://tieba.baidu.com/p/4855113094?pn=1',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',
        'http://tieba.baidu.com/p/4855113094?pn=4',

        'http://tieba.baidu.com/p/4855113094?pn=2',  # 这四个URL已爬，将被去重
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',

        'http://tieba.baidu.com/p/4855113094?pn=5',
        'http://tieba.baidu.com/p/4855113094?pn=6',
    ]

    def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parse1)

    def parse1(self, response):
        print 'We Get: %s' % response.url


class Spider2(RedisSpider):
    name = "Spider2"
    redis_key = "scrapyWithBloomfilter_demo:start_urls"
    start_urls = [
        'http://tieba.baidu.com/p/4855113094?pn=1',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',
        'http://tieba.baidu.com/p/4855113094?pn=4',

        'http://tieba.baidu.com/p/4855113094?pn=2',  # 这四个URL已爬，将被去重
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',

        'http://tieba.baidu.com/p/4855113094?pn=5',
        'http://tieba.baidu.com/p/4855113094?pn=6',
    ]

    def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parse1)

    def parse1(self, response):
        print 'We Get: %s' % response.url


```

# 16 redis加入windows服务自启动 
- 安装redis之后,在命令行窗口中输入 redis-server redis.windows.conf 启动redis
关闭命令行窗口就是关闭 redis。
- redis作为windows服务启动方式,**redis-server --service-install redis.windows.conf**
- 启动服务：**redis-server --service-start**
- 停止服务：**redis-server --service-stop**

# 17 有关redis的数据类型和抽象 [详细](https://redis.io/topics/data-types-intro)
redis绝不是一个平淡的key-value存储，它实际上是数据结构服务器，支持不同类型的值。
- Binary-safe strings 
- Lists 是string元素的集合，通过顺序插入
- Sets 是string元素的几何，未排序，唯一
- Sorted sets 与Sets类似，但每一个string元素都与一个浮点数相关，成为score；通常通过score完成了排序，可以进行范围的检索，如top10或bottom10
- Hshes 它的field和value都是string，与Ruby或Python的hashes类似
- Bit arrays (or simply bitmaps) 通过特别的命令处理string的值类似an array of bits，可以设置或清除无效的位，统计所有位等于1的集合等
- HyperLogLogs 用于估计set的基数
### redis keys （key size最大为512MB）
所有的redis keys都是binary safe，可以使用任意的二进制序列表示一个key，空的string也是一个有效的key
- 规则1： 太长不好，可进行哈希，如SHA1
- 规则2： 太短不好，使用"user:100:followers"代替"u1000flw"的可读性更好
- 规则3： 尝试建立一种模式，如"object-type:id","comment:1234:reply-to"
### redis Strings
是Memcached中的唯一数据类型，string data type对大量的例子来说非常好用，如HTML或pages；通过set和get命令可以设置和检索string的值。key相同时再次set会替换values，即使与key相关的是一个non-sting值。**set 即赋值**。value的大小也不能超过512MB。
```
set mykey somevalue
get mykey
incr mykey # 自增一
```
incr命令将string value解析为integer，每次增长一，完成赋新值；还有相似的命令incryby, decr, decrby;如果有client1和client2都incr，不会出现同时的情况，最后的结果是增加了2，因为inrc执行的条件是：while all the other clients are not executing a comand at the same time.还有一个命令getset可以实现赋新值，但返回旧的结果。也可以一次设置多个value或取多个value
```
mset a 10 b 20 c 30
mget a b c 
```
查看key是否在，用exists mykey； 删除用 del mykey；返回值都是0或1。查看类型type mykey,没有的话返回none。
可以设置keys的**期限**，可以是秒或毫秒，到期后的key将销毁，等同于执行了del命令
```
set key some-value
expire key 5 
get key
"some-value"
get key (after some time)
(nil)
```
还可以在set是设置期限，通过ttl查看，通过persist移除之前的设置并改为永久;以上的期限都是秒级，[有关毫秒级的查文档](https://redis.io/commands/set)。
```
set key 100 ex 10
ttl key
```
# 18 redis Lists
List就是一个序列或顺序元素的几何，python中的list是数字array实现的，redis和ruby中的list是用Linked List实现的，还是有不同的。
```
rpush mylist A        # 增加
lpush mylist B        # 增加
lrange mylist 0 -1    # 检索，-1表示倒数第一个
rpush mylist 1 2 3 4 5 "boo bar"     # 一次增加多个也是可以的
```
弹出元素pop，返回弹出结果，同时实现在list中删除；空的list表示为nil；redis返回一个NULL值表示list为空。
```
rpush mylist a b c
rpop mylist
```
Lists可以用于大量的tasks，下面两个非常有代表性：
- 在社交网络中，记录最近用户提交的更新
- 进程间通信，生产中消费者模型，生产中将元素放入list，消费者（worker）消费并执行操作。redis有专门的list命令使得这种情况更加稳定和高效。
也可对list进行限制，Capped lists，只记录最新的N个items，**丢弃**之前所有的。
```
rpush mylist 1 2 3 4 5        # (integer) 5
ltrim mylist 0 2              # OK
lrange mylist 0 -1            # [1, 2, 3]
```
# 19 Blocking operations on list 阻断操作
list的特性可以使之很容易实现一个队列queues，如生产中-消费者中使用lpush添加元素，使用rpop取和处理元素；当list空的时候rpop返回NULL，但消费者会不断的等和尝试。
redis使用**brpop**和**blpop**替代之前的rpop和lpop，这样当list为空时进行阻断，只有一个新的元素进入后或一个用户定义的时间timeout了才返回给调用者。[BRPOP](https://redis.io/commands/brpop) **返回长度LLEN**
```
brpop task 5  # 表示增加了5秒的等待，0表示永久等待
```
# 20 Redis Hashes
命令hmset设置hash中多个值，hget检索一个值，hmget返回为数组。[更多](https://redis.io/commands#hash)
```
hmset user:1000 username antirez birthyear 1977 verified 1  # OK
hget user:100 username      # "antirez"
hget user:100 birthyear     # "1977"
hgetall user:100
hincrby user:1000 birthyear 10  # (integer) 1987
hincrby user:1000 birthyear 10  # (integer) 1997K
```
# 21  Redis Sets
```
sadd myset 1 2 3    # (integer) 3
smembers myset      # 3 1 2 无序
sismember myset 3   # (integer)1 查看在不在
sismember myset 30  # (integer)0 
sinter tag:1:news tag:2:news tag:3:news     
```

# 22 发送邮件 yagmail
```
import yagmail
mail = yagmail.SMTP(users='', passwords='', host='smtp.163.com', port='25')
mail.send(to='@', subject='主题', contents='主体')
```

# 23 cmder和cmd说再见
- Cmder is a software package created out of pure frustration over the absence of nice console emulators on Windows. It is based on amazing software, and spiced up with the Monokai color scheme and a custom prompt layout, looking sexy from the start. 
- Think about cmder more as a software package than a separate app. All the magic is happening through[ Conemu](https://conemu.github.io/). With enhancements from [Clink](https://mridgers.github.io/clink/).
- Git and others.Oooh yes! If you decide to use the slightly bigger  git-for-windows version, you will have all Unix commands ready in PATH so that you can git init or cat instantly on every machine.
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/2A1FAF3190A9475DBA7D45B2A51C8796/1719)
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/C0610920EE5844D29545B119F61D160F/1716)
- 问题![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/DD125498941B439C86C5CCEF7CEBD08C/1728)
- 解决：解压路径不能有空格
- [更详细内容](http://www.cnblogs.com/jadeboy/p/5132423.html)
### 小技巧
工作的时候经常需要在各个目录之间跳转，可以通过环境变量对目录进行缩写，方便地在多个目录直接切换。
在 ~/.bashrc 添加：
```
export wd="/d/Projects/MyProject/git"
export doc="/d/Projects/documents/"
```
以后只需要用 cd $wd, cd $doc 即可进入对应目录

# 24 python3中使用time.tzname输出乱码
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/60A1BCD8D0ED4CC3BA0B857911A24000/1702)


# 25 Error WinError 10013 以一种访问权限不允许的方式做了一个访问套接字的尝试
![# 97](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/970AADF9127F4168A73AD004E3792F51/1801)

# 26 git多站点多用户情况下SSH配置
![# 98 ](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/982D6BE1CE974C8989EFF6E67E90D657/1803)

# 27 xpath 小技巧
![99](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/DE85DB66FDF2407EA4040668E4DDABCB/1821)

# 28 Picture控件 IplImage Mat
![# 100](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/99F8E0283312400F859D01A97E70E9B7/1805)