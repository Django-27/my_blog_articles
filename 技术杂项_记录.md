### 启动一个简单的服务器
    Python2 python -m SimpleHTTPServer 8080
    Python3 python -m http.serve
### 正则表达式匹配代理
    r = re.compile("[0-9]+(?:\.[0-9]+){3}:\d{2,4}")
    r.findall('123.45.56.78:80')
    ['123.45.56.78:80']
##### (?:pattern)
匹配 pattern 但不获取匹配结果，也就是说这是一个非获取匹配，不进行存储供以后使用。这在使用 "或" 字符 (|) 来组合一个模式的各个部分是很有用。例如， 'industr(?:y|ies) 就是一个比 'industry|industries' 更简略的表达式。那么(?:13\d|15\d)也就是要在这里面选择匹配13+0~9的数字一个或15+0~9的数字
##### \\*
因为\*是正则表达式里的特殊字符，所以，这里要匹配字符串里的*时，得用转义字符“\”
那么\\\*{3}就是要匹配“****”
#### windows关机命令
    shutdown -s -f # 关机+强制
    shutdown -a    # 取消
### list Comprehensions 列表推导式 和 生成器表达式
这是一种将for if 赋值放到单一语句中的一种方法，即通过一个表达式对一个列表做映射或过滤操作

    nums = [1, 4, -1, 7, 9]
    filterd_and_squared = [x**2 for x in nums if x > 0]

负面效应：一次加载到内存中；可以使用生成器解决，有着几乎相同的语法，但生成器表达式被圆括号包围

    filter_and_squared = (x**2 for x in nums if x > 0)
    for item in filter_and_squared:
        print(item)
    print(list(filter_and_squared)

   
### 构建路由规则

一个 web 应用不同的路径会有不同的处理函数，路由就是根据请求的 URL 找到对应处理函数的过程; flask中有两种方法（还有一种非常规方法app.url_map）：

 1. @app.route() 生成器

    @app.route('/')
    def hello():
        return "hello world"
 2. app.add_url_rule 方法签名，包括URL规则字符串和对于URL的处理函数

    def hello():
        return "hello world"
    app.add_url_rule('/', 'hello', hello)

事实上，flask核心的路由逻辑在werkzeug中实现，实现URL到endpoint的转换；而endpoint和view function之间的匹配关系flask存放到了字典中

### 短连接bit.ly不能访问
通过在chrome中安装插件Tampermonkey可以解决；如http://bit.ly/28UUgW3 ；安装方式是通过在插件界面新建文本，保存js代码，之后便实现了安装
![][1]插件代码选择由Greasy Fork下载
![enter image description here][2]
  [1]: images/2017-02-15-18-QQ%E6%88%AA%E5%9B%BE20170215094758.png "QQ截图20170215094758.png"
  [2]: images/2017-02-15-14-QQ%E6%88%AA%E5%9B%BE20170215095402.png "QQ截图20170215095402.png"
### 字符类型检测
以前是chardet,现在是cchardet,提供更快的检测,安装cchardet包含了chardet

    import cchardet as chardet 
    with open('tmp.txt', 'rb') as f:
        print(chardet.detect(f.read))
    >> {'confidence': **, 'encoding': **}
    
    print(charset.detect(requests.get('http://www.baidu.com').text))
    >> ValueError: Expected a bytes object, not a unicode object
    print(charset.detect(requests.get('http://www.baidu.com').content))
    >> {'encoding': 'utf-8', 'confidence': 0.99}
    
    '¼à¿ØÖÐÐÄ'.encode('latin-1').decode('gbk')
    >> '监控中心'

### 笔记由小书匠迁移到有道云（图片问题）
小书匠的问题是不稳定，有时都不能编辑，侧边栏在编辑的时候也不跟着走
有道云在markdown中添加本地图片比较麻烦
- 先将本地图片上传，分享，打开后右键找到图片的地址，插入时使用这个图片地址即可
- 另一种方法是新建一个笔记，这样所有的图片都可以放进去，也是分享，打开分享链接，找对对应的图片，右键复制就得到了图片的地址
- speed_27  speed1478
 

### 布隆滤波器（Bloom Filter）
- 由波顿·布隆与1970年提出，只需要哈希表1/8到1/4的大小就可解决；它实际上是一个很长的**二进制向量**和一系列**随机映射函数**。例如：存储16亿个电子邮件的地址，每个地址用16位表示（可能8位既可以表示一个电子邮箱的信息指纹，但考虑哈希的效率为50%，使用16位），将是16亿二进制（比特），即2亿字节的向量，然后将这**16亿**二进制位全部清零；对于每一个电子邮件地址**X**,用8个不同的随机数产生器（F1，F2，··，F8）产生8个信息指纹(f1,f2,··，f8)，再使用一个随机数产生器**G**把这8个信息指纹映射到1-16亿中的8个自然数g1,g2,··,g8，**最后**将这8个位置的二进制全部设置为1。对1亿个点至邮件地址都这样处理后，一个针对这些电子邮件地址的布隆滤波器就建好了。
- 同理，检测一个可以的电子邮件地址是否在黑名单中，即执行上面相同的操作后看对应的8个二进制位是不是都是1.
- 布隆滤波器决不会漏掉黑名单中的任何一个可以地址，但有可能将一个不在黑名单中的误判，上面的这个例子中误识别率在万分之一以下。好处是快速、省空间，但有一定的误识别率，常见的补救方法是再建一个小的白名单，存储那些可能被误判的邮件地址。
- 有关误识别率统计（假阳性false positive rate）[曹培](http://pages.cs.wisc.edu/~cao/papers/summary-cache/node8.html) m表示滤波器有m比特，里面有n个元素（插入n个元素后），每个元素对应的是k个信息指纹的哈希函数。Here is [a very nice visualization of how it works](https://www.jasondavies.com/bloomfilter/)![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/7DD4E0C67D854D6480E497A536AA2F7A/1256)


- pip install cython
- pip install pybloomfiltermmap3

```
>>> import pybloomfilter
>>> fruit = pybloomfilter.BloomFilter(100000, 0.1, '/tmp/words.bloom')
>>> fruit.update(('apple', 'pear', 'orange', 'apple'))
>>> len(fruit)
3
>>> 'mike' in fruit
False
>>> 'apple' in fruit
True
```
(pybloom module) |  (this module)
-|-
pybloom load took 0.76436 s/run | pybloomfilter load took 0.05423 s/run
pybloom tests took 0.16205 s/run | pybloomfilter tests took 0.00659 s/run
Errors: 0.25% positive 0.00% negative | Errors: 0.26% positive 0.00% negative


### 基于Redistribute的Bloomfilter URL去重
- 数据量不大时可以直接放在内存中去重，如Python的set()进行；当去重数据需要持久化时可以使用redistribute的set数据结构；当数据量再大时可以使用不同的加密算法先将字符串压缩成16/32/40个字符，然后使用上面的两种方法；
- 当数据量达到亿数量级时，内存有限必须使用‘位’来去重，才能满足需求；然而Bloomfilter运行在一台机器上的内存上，不方便持久化，机器down掉就什么也没有了，也不方便分布式爬虫的统一去重，可以在redis上申请内存进行Bloomfilter，即可解决上面的问题。(redis总string一个最大为512M，在多的数据就要分块了) 
```
# encoding=utf-8              # Py2

import redis
from hashlib import md5


class SimpleHash(object):
    def __init__(self, cap, seed):
        self.cap = cap
        self.seed = seed

    def hash(self, value):
        ret = 0
        for i in range(len(value)):
            ret += self.seed * ret + ord(value[i])
        return (self.cap - 1) & ret


class BloomFilter(object):
    def __init__(self, host='localhost', port=6379, db=0, blockNum=1, key='bloomfilter'):
        """
        :param host: the host of Redis
        :param port: the port of Redis
        :param db: witch db in Redis
        :param blockNum: one blockNum for about 90,000,000; if you have more strings for filtering, increase it.
        :param key: the key's name in Redis
        """
        self.server = redis.Redis(host=host, port=port, db=db)
        self.bit_size = 1 << 31  # Redis的String类型最大容量为512M，现使用256M
        self.seeds = [5, 7, 11, 13, 31, 37, 61]
        self.key = key
        self.blockNum = blockNum
        self.hashfunc = []
        for seed in self.seeds:
            self.hashfunc.append(SimpleHash(self.bit_size, seed))

    def isContains(self, str_input):
        if not str_input:
            return False
        m5 = md5()
        m5.update(str_input)
        str_input = m5.hexdigest()
        ret = True
        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)
        for f in self.hashfunc:
            loc = f.hash(str_input)
            ret = ret & self.server.getbit(name, loc)
        return ret

    def insert(self, str_input):
        m5 = md5()
        m5.update(str_input)
        str_input = m5.hexdigest()
        name = self.key + str(int(str_input[0:2], 16) % self.blockNum)
        for f in self.hashfunc:
            loc = f.hash(str_input)
            self.server.setbit(name, loc, 1)


if __name__ == '__main__':
""" 第一次运行时会显示 not exists!，之后再运行会显示 exists! """
    bf = BloomFilter()
    if bf.isContains('http://www.baidu.com'):   # 判断字符串是否存在
        print 'exists!'
    else:
        print 'not exists!'
        bf.insert('http://www.baidu.com')    
```


### scrapy_redis去重优化                  
- scrapy默认是开启了去重的，scrapy_redis会用自己的scheduler替代scrapy框架的scheduler进行任务调度；在scrpay_redis模块下查看scheduler.py源码。
- •在open()方法中有句self.df = RFPDupeFilter(...)，可见去重应该是用了RFPDupeFilter这个类；再看下面的enqueue_request()方法，里面有句if not request.dont_filter and self.df.request_seen(request):return，看来self.df.request_seen()这就是用来去重的了。
- •按住Ctrl再左键点击request_seen查看它的代码，可看到下面的代码：
```
def request_seen(self, request):
    fp = request_fingerprint(request)
    added = self.server.sadd(self.key, fp)
    return not added1
```
- 是否可用Bloomfilter进行优化？ 
 以上步骤可以看出，我们只要在这个request_seen()方法上面动些手脚即可。由于现有的七亿多去重数据存的都是这个fingerprint，所有Bloomfilter去重的对象仍然是request对象的fingerprint。更改后的代码如下：
```
def request_seen(self, request):
    fp = request_fingerprint(request)
    if self.bf.isContains(fp):  # 如果已经存在
        return True
    else:
        self.bf.insert(fp) # self.bf是类Bloomfilter()的实例化
        return False1 
```
#### 优化的思路和代码就是这样；以下将已有的七亿多的去重数据转成Bloomfilter去重
- 内存将爆，动作稍微大点机器就能死掉，更别说Bloomfilter在上面申请内存了。当务之急肯定是将那七亿多个fingerprint导出到硬盘上，而且不能用本机导，并且先要将redis的自动持久化给关掉。
- 因为常用Mongo，所以习惯性首先想到Mongodb，从redis取出2000条再一次性插入Mongo，但速度还是不乐观，瓶颈在于MongoDB。（猜测是MongoDB对_id的去重导致的，也可能是物理硬件的限制）
- 后来想用SSDB，因为SSDB和Redis很相似，用list存肯定速度快很多。然而SSDB唯独不支持Centos7，其他版本的系统都可。。
- 最后才想起来用txt，这个最傻的方法，却是非常有效的方法。速度很快，只是为了防止读取时内存不足，每100万个fingerprint存在了一个txt，四台机器txt总共有七百个左右。
- fingerprint取出来后redis只剩下一千多万的Request种子，占用内存9G+。然后用Bloomfilter将txt中的fingerprint写回Redis，写完以后Redis占用内存25G，开启redis自动持久化后内存占用49G左右。
- 如果待去重的数据量比较大，需要修改scrapy_redis/dupefilter.py中第14行的blockNum值，默认blockNum=1。Bloomfilter算法是有漏失概率的（即不存在的会误判为存在），在保证漏失率小于万分之一的情况下，一个blockNum可满足7千万条数据的去重，一个blockNum占用256M内存（注意Linux如果开了自动持久化，redis占用内存会加倍）。

```
from scrapy import Spider, signals
from scrapy.exceptions import DontCloseSpider

from . import connection

class RedisMixin(object):
    """Mixin class to implement reading urls from a redis queue."""
    redis_key = None  # use default '<spider>:start_urls'

    def setup_redis(self):
        """Setup redis connection and idle signal.

        This should be called after the spider has set its crawler object.
        """
        if not self.redis_key:
            self.redis_key = '%s:start_urls' % self.name

        self.server = connection.from_settings(self.crawler.settings)
        # idle signal is called when the spider has no requests left,
        # that's when we will schedule new requests from redis queue
        self.crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)
        self.crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)
        self.log("Reading URLs from redis list '%s'" % self.redis_key)

    def next_request(self):
        """Returns a request to be scheduled or none."""
        url = self.server.lpop(self.redis_key)
        if url:
            return self.make_requests_from_url(url)

    def schedule_next_request(self):
        """Schedules a request if available"""
        req = self.next_request()
        if req:
            self.crawler.engine.crawl(req, spider=self)

    def spider_idle(self):
        """Schedules a request if available, otherwise waits."""
        self.schedule_next_request()
        raise DontCloseSpider

    def item_scraped(self, *args, **kwargs):
        """Avoids waiting for the spider to  idle before scheduling the next request"""
        self.schedule_next_request()


class RedisSpider(RedisMixin, Spider):
    """Spider that reads urls from redis queue when idle."""

    def _set_crawler(self, crawler):
        super(RedisSpider, self)._set_crawler(crawler)
        self.setup_redis()

```

```
# encoding=utf-8
# ---------------------------------------
#   版本：0.1
#   日期：2016-11-10
#   作者：九茶 http://blog.csdn.net/bone_ace?viewmode=contents
# ---------------------------------------

from scrapyWithBloomfilter_demo.scrapy_redis.spiders import RedisSpider
from scrapy.http import Request


class Spider1(RedisSpider):
    name = "Spider1"
    redis_key = "scrapyWithBloomfilter_demo:start_urls"
    start_urls = [
        'http://tieba.baidu.com/p/4855113094?pn=1',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',
        'http://tieba.baidu.com/p/4855113094?pn=4',

        'http://tieba.baidu.com/p/4855113094?pn=2',  # 这四个URL已爬，将被去重
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',

        'http://tieba.baidu.com/p/4855113094?pn=5',
        'http://tieba.baidu.com/p/4855113094?pn=6',
    ]

    def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parse1)

    def parse1(self, response):
        print 'We Get: %s' % response.url


class Spider2(RedisSpider):
    name = "Spider2"
    redis_key = "scrapyWithBloomfilter_demo:start_urls"
    start_urls = [
        'http://tieba.baidu.com/p/4855113094?pn=1',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',
        'http://tieba.baidu.com/p/4855113094?pn=4',

        'http://tieba.baidu.com/p/4855113094?pn=2',  # 这四个URL已爬，将被去重
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=2',
        'http://tieba.baidu.com/p/4855113094?pn=3',

        'http://tieba.baidu.com/p/4855113094?pn=5',
        'http://tieba.baidu.com/p/4855113094?pn=6',
    ]

    def start_requests(self):
        for url in self.start_urls:
            yield Request(url=url, callback=self.parse1)

    def parse1(self, response):
        print 'We Get: %s' % response.url


```

