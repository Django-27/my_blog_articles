---
title: 技术杂项_2_记录(已完成)
date: 2017-04-14
tags:
---

# 1 requests中text和content的区别
```
r.text is the content of the response in unicode.
r.content is the content of the response in bytes.
```
# 2 cookies中的_ga, _gat是什么意思
```
cookies = {
    '_ga': 'GA1.2.2003702965.1486066203',
    '_gat': '1',
}
```
#### 全称：Google Analytic's javascript to be used on a web page for web-tracking.
#### 同理：Hm_lpvt_, Hm_lvt_, 为百度统计

```
<script type="text/javascript">  
        var _gaq = _gaq || [];//定义GA变量数组。  
        _gaq.push(['_setAccount', 'UA-24479793-2']);//设置本跟踪代码所对应的Google帐户。  
        _gaq.push(['_trackPageview']);//定义按页面跟踪  
        (function () {//定义匿名的执行方法  
            var ga = document.createElement('script');//定义GA的脚本Dom对象。到时候会appendChild到Document中  
            ga.type = 'text/javascript';//不解释  
            ga.async = true;//定义GA数据传输方式为异步传输。  
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';//定义GA的JS源路径，自动取的，主要是做了一个协议判断，意味着GA可以跟踪htts网页和ssl网页，当你 的页面是http时就去http://www.google-analytics.com/ga.js取代码。当你是https页面时就去https://www.google-analytics.com/ga.js取代码。也可以指定为本地服务器目录/ga.js即本地加载ga.js代码。  
            var s = document.getElementsByTagName('script')[0];  
            s.parentNode.insertBefore(ga, s);//添加GA代码  
        })();  
    </script> 
```
# 3 opencv不支持打开gif，采用SimpleCV和matplotlib、PIL替代
gif分为动态的和静态的，返回的验证码保持为gif文件，需要去图片查看器打开；能不能使用opencv直接显示呢，不行，到3.6版还是不支持此格式；SimpleCV到时可以，2015.4.7停更，用起来也麻烦；使用matplotlib实现了gif的显示。
```
import matplotlib.pyplot as plt
img=plt.imread('captcha.gif')       
plt.imshow(img) 
plt.show()
```
# 4 收集chrome插件
Octotree,在浏览github的时候，添加侧边栏，避免了从选目录、文件的麻烦。
![104](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/45AA5DA3F9CE4829A9F0D35C4FA46F44/2102)
# 5  .gitignore 使用小记
给项目添加一个 .gitignore 文件，从而使那些不该纳入版本控制的文件能够被 Git 排除在外的，如Python中的.pyc文件，可以自定义也可以参考[gitignore.io](https://github.com/joeblau/gitignore.io)提供的对于文件。
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/A21DEDC95ACC453AABF0DF340DB43EEA/2109)


# 5 生成自己的词云 [link](http://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html#wordcloud.WordCloud)
```
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import jieba

text_from_file_with_apath = open('星辰变.txt').read()
wordlist_after_jieba = jieba.cut(text_from_file_with_apath, cut_all = True)
wl_space_split = " ".join(wordlist_after_jieba)
my_wordcloud = WordCloud().generate(wl_space_split)

plt.imshow(my_wordcloud)
plt.axis("off")
plt.show()
```
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/B9B3A8A207E54FA5808053B2BA55B763/2143)
```
my_wordcloud = WordCloud(font_path="C:\Windows\fonts\SIMYOU.TTF").generate(wl_space_split)  
```
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/E48325A2CEBB44F380B202B98655E8D1/2175)


# 6 代理地址
    'http://cn-proxy.com',
    'http://www.xicidaili.com',
    'http://www.kuaidaili.com/free',
    'http://www.proxylists.net/?HTTP',
    # www.youdaili.net的地址随着日期不断更新
    'http://www.youdaili.net/Daili/http/4565.html',
    'http://www.youdaili.net/Daili/http/4562.html',
    'http://www.kuaidaili.com',
    'http://proxy.mimvp.com'
其中 http://cn-proxy.com 已经挂掉不能访问了；
 其中 http://www.proxylists.net/?HTTP 和 www.youdaili.net 的代理是文本格式，能够直接按照给出的正则表达式匹配。
 剩余的 www.xicidaili.com 、http://www.kuaidaili.com/free 、http://www.kuaidaili.com 给出代理的格式都是表格形式， IP 和端口并不在一起，使用提供的正则匹配不到。一种解决方案是用 bs 匹配 IP 后再获取兄弟节点的端口号：
```
bsObj = BeautifulSoup(html, 'html.parser')
ipList = bsObj.findAll('td', text=re.compile('[0-9]+(?:\.[0-9]+){3}'))
for each in ipList:
    ip = each.get_text()
    port = each.next_sibling.next_sibling.get_text() #因为换行的原因要取两次兄弟节点
    proxy = ip + ':' +port
```
最后一个 http://proxy.mimvp.com 就更扯了…端口一列直接是给出的图片…

# 7 JS代码String.fromCharCode（）
String.fromCharCode()方法用于把一个或多个 Unicode 值转换为（大写）字符串，并返回该字符串。
```
<script type="text/javascript">
    String.fromCharCode(65,66,67)   #  下例返回字符串 "ABC"
</script>
```
```
car1 = {"type":'Mazda', "model":5, "color":'white'}
attr = execjs.compile("""
    function car_type(x) {var temp = x; return temp.type;} """)
print(attr.call("car_type", car1))   # 返回:Mazda
```
```
from selenium import webdriver
url = "http://www.taobao.com/"
browser = webdriver.PhantomJS()
browser.get(url)
input = browser.find_element_by_xpath("//input[@id='q']")
bnt = browser.find_element_by_xpath("//button[@class='btn-search']")
input.send_keys("watch")
bnt.submit()
```
```
my_js_function = 'function my_function(a){var b=a.split("$");var c="";for(i=0;i<=b.length-1;i++){if(!(i%2)){c+=b[i]}}var b=c.split("");c="";for(i=0;i<=b.length-1;i++){if(i%2){c+=b[i]}}return c}'
compiled_function = ExecJS.compile(my_js_function)
compiled_function.eval('my_function("heyhey")')   #  => ehy
```

# 8 获取代理IP， 快代理， PyExecJs vs Selenium
``` 20170329 
file = open('tmp.html', 'r').read()
ret1 = re.findall(r'gv\((.*?)\)', file)  # ret1的结果: ['193', 'RM']
ret2 = 'var  ' + re.findall(r'var (.*?) </script>', file, re.DOTALL)[0].replace(ret1[1], ret1[0])[:25] + ' return po;'  # ret2的结果：拼接js代码，上图中的8行到25行，加上最后的return语句返回结果
ret3 = execjs.exec_(ret2)  # "document.cookie='_ydclearance=1ed2225a7c109a04e94ebde3-c605-43f4-a5ff-8293304327ce-1490766671; expires=Wed, 29-Mar-17 05:51:11 GMT; domain=.kuaidaili.com; path=/'; window.document.location=document.URL"

ret3.split(';')[0].split('=')  # ['document.cookie', "'_ydclearance", '1ed2225a7c109a04e94ebde3-c605-43f4-a5ff-8293304327ce-1490766671']
```
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/FB691D943FBA470FBEFA18F88E263456/2482)
#### 补充，js代码的函数名，参数名，给定的参数都是变得，上面指定gv还是有问题的
```
ret1 = re.findall(r'function (.*?)\(', file)[0]  # 取得函数名
Out[148]: 'jq'
ret1 = re.findall(r'%s\((.*?)\)'%ret1, file)  # 取得参数名，参数值；正则中包含变量，使用格式化字符串相关
Out[150]: ['35', 'RD']

ret2 = 'var  ' + re.findall(r'var (.*?) </script>', file, re.DOTALL)[0][:-25].replace(ret1[1], ret1[0]) + ' return po;'
                
ret3 = execjs.exec_(ret2)
k = ret3.split(';')[0].split("'")[1].split('=')[0]
v = ret3.split(';')[0].split("'")[1].split('=')[1]
self.session.cookies.update({k: v})  # {'_ydclearance': '0ac1fa903ebbfd9ef7b1bd75-7e1f-4461-921c-c4b2afa95a10-1490785013'}
```
#### 最后的目地就是更新了一下cookies，之后的请求就没有问题了。（注意在js中补充的return是一个要点）代码在github[地址](https://github.com/Django-27/my_spider/blob/master/proxy_kuaidaili.py)
### 相关js代码：
```
<html>

<body>
    <script language="javascript">
        window.onload = setTimeout("kx(222)", 200);

        function kx(SE) {
            var qo, mo = "",
                no = "",
                oo = [0x65, 0xc6, 0x1e, 0xa6, 0x6d, 0x4d, 0x17, 0xdf, 0x06, 0xf0, 0xcb, 0xe9, 0x72, 0x7c, 0x82, 0x22, 0xab, 0xf4, 0xb9, 0x79, 0x61, 0x62, 0x6e, 0xb5, 0x9e, 0x38, 0xc0, 0xa6, 0x6f, 0xb7, 0xce, 0x75, 0xbd, 0xe7, 0x70, 0x3b, 0x04, 0xc3, 0xa2, 0xc1, 0x97, 0xdf, 0x9e, 0xc7, 0x50, 0x56, 0x7f, 0x9e, 0x9d, 0x86, 0x78, 0xd6, 0xd5, 0x78, 0x37, 0xe8, 0xcd, 0x70, 0xcf, 0xb8, 0x00, 0x40, 0x7f, 0x28, 0xc6, 0x38, 0x57, 0x40, 0xfd, 0xe6, 0x61, 0xa4, 0xc3, 0x4c, 0x0a, 0x1f, 0xfd, 0x00, 0xc5, 0x88, 0x98, 0x56, 0xb5, 0x18, 0x56, 0x7d, 0xff, 0xbd, 0xc0, 0x7f, 0xe3, 0xa2, 0x6b, 0xc9, 0x29, 0x45, 0x2e, 0x90, 0xef, 0x4f, 0x89, 0x72, 0x51, 0x13, 0x51, 0x0e, 0xf0, 0x30, 0x8e, 0xed, 0xa9, 0x0c, 0xaa, 0xe9, 0x49, 0x00, 0x60, 0xdf, 0x40, 0x05, 0x7d, 0xe8, 0x4f, 0x98, 0x3f, 0x2f, 0xf7, 0x7e, 0x3e, 0x49, 0x22, 0xea, 0xd3, 0xb1, 0x12, 0x2a, 0xcc, 0x0c, 0xc9, 0x97, 0x8f, 0xd7, 0x7e, 0x40, 0x7e, 0x4d, 0x50, 0xb0, 0xee, 0xad, 0x50, 0xef, 0xae, 0xcd, 0x6d, 0x60, 0xfe, 0x5e, 0xbe, 0xcb, 0xbf, 0x89, 0x78, 0xf7, 0x58, 0x0c, 0xf4, 0xfa, 0xc4, 0x09, 0xe1, 0x2b, 0x51, 0x11, 0x33, 0xb8, 0x3e, 0x05, 0x4d, 0x96, 0x92, 0x7b, 0xc3, 0x09, 0xf2, 0x76, 0xbf, 0xdd, 0x66, 0x70, 0x05, 0xce, 0x4e, 0xae, 0x15, 0x52, 0x9a, 0x81, 0xea, 0xaa, 0xf3, 0xf1, 0xf2, 0x72, 0xd2, 0x15, 0x20, 0x65, 0x8f, 0x78, 0x39, 0x43, 0x4a, 0x6c, 0x51, 0xc6, 0xd0, 0x55, 0x20, 0xe5, 0xd3, 0x9c, 0xc2, 0xa9, 0xcb, 0xe0, 0xc6, 0xd0, 0x55, 0x9d, 0x51, 0x3c, 0x81, 0x8b, 0xb1, 0xec, 0xac, 0x91, 0x9b, 0x24, 0x84, 0x4b, 0x11, 0xd9, 0x04, 0xbe, 0xa5, 0xc7, 0x92, 0x3d, 0x38, 0xcd, 0x3b];
            qo = "qo=251; do{oo[qo]=(-oo[qo])&0xff; oo[qo]=(((oo[qo]>>2)|((oo[qo]<<6)&0xff))-58)&0xff;} while(--qo>=2);";
            eval(qo);
            qo = 250;
            do {
                oo[qo] = (oo[qo] - oo[qo - 1]) & 0xff;
            } while (--qo >= 3);
            qo = 1;
            for (;;) {
                if (qo > 250) break;
                oo[qo] = ((((((oo[qo] + 118) & 0xff) + 153) & 0xff) << 5) & 0xff) | (((((oo[qo] + 118) & 0xff) + 153) & 0xff) >> 3);
                qo++;
            }
            po = "";
            for (qo = 1; qo < oo.length - 1; qo++)
                if (qo % 5) po += String.fromCharCode(oo[qo] ^ SE);
            eval("qo=eval;qo(po);");
        }
    </script>
</body>

</html>
```
### 采用Selenium解决
- 由于这个是html中包含js代码，也不能一起执行html，不能直接实现script中间的，都报错
- 最后，手动指定 参数名和参数取值，并且去掉最后的eval语句，才可以拿到结果
- 这里Selenium没能够跳转页面，也是卡在了这里，多次请求还是这个页面；这种情况下requests比Selenium更优吧（消耗更小）。
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/A70D471D0F4344D6B91BB91BCCA40888/2666)

# 9  获取代理IP， 快代理， Selenium with PhantomJS

# 10 requests离线pdf或其他大文件
#### 重点是设置：stream=True
```python
import requests

url = 'http://disclosure.szse.cn/finalpage/2016-04-25/1202231488.PDF'
r = requests.get(url, stream=True)
chunk_size = 2000  # bytes
if r.status_code == 200:
    with open('/1202231488.pdf', 'wb') as f:
        for chunk in r.iter_content(chunk_size):
            f.write(chunk)
    print('finished')
```

# 11 Firfox's about:config  in chrom
about:about or chrome://about or about:flags
Chromium是Chrom的开源版，也可以理解Chromium是Chrome的测试版（收集信息（国家、安装次数）），稳定之后会进行Chome升级，哈哈。
Chromium提供命令行的控制，[详见](http://peter.sh/experiments/chromium-command-line-switches/)。
#### FireFox的这个东西，可能也是很多人选择的原因吧；例如：结合Selenium的profile对浏览器控制。
C:\WINDOWS\system32\drivers\etc\hosts 
```
from selenium import webdriver

firefox_profile = webdriver.FirefoxProfile()                         # 初始化化一个firefox_profile实例
firefox_profile.set_preference('permissions.default.image', 2)       # 不下载和加载图片
firefox_profile.set_preference('permissions.default.stylesheet', 2)  # 禁用样式表文件
firefox_profile.set_preference('javascript.enabled', False)          # 禁止Javascript的执行
firefox_profile.update_preferences()

firfox = webdriver.Firefox(firefox_profile)
firfox.get('http://www.baidu.com')
page = firfox.page_source  # 获取网页渲染后的源代码
firfox.quit()  # 检查发现，退出后本机about：config文件中对应参数没有被改变。
```
#### WebDriverException: Message: 'geckodriver' executable needs to be in PATH.
- selenium 3.x开始，webdriver/搜索firefox/webdriver.py的__init__中，executable_path="geckodriver"；而2.x是executable_path="wires"
- firefox 47以上版本，需要下载第三方driver，即geckodriver；在[link](https://github.com/mozilla/geckodriver)下载到任意电脑任意目录，解压后将该路径加入到PC的path（针对windows）即可。 
```
from selenium import webdriver

browser = webdriver.PhantomJS(executable_path='C:/Users/qiyuan/Desktop/phantomjs.exe')  # 无界面浏览器
browser.get('http://item.jd.com/1312640.html')
page = browser.page_source  # open('tmp_jd.html', 'w', encoding='utf-8').write(browser.page_source)

# browser.find_element_by_tag_name("div").text
# selenium在webdriver的DOM中使用选择器来查找元素，名字直接了当，
# by对象可使用的选择策略有：id,class_name,css_selector,link_text,name,tag_name,tag_name,xpath等等

```

# 12 Selenium with Python 控制浏览器行为
```python 
from selenium import webdriver          # 成功打开本机FireFox浏览器，并进行搜索操作。
from selenium.webdriver.common.keys import Keys

driver = webdriver.Firefox()
driver.get("http://www.python.org")

elem = driver.find_element_by_name("q")  # 找到输入框
elem.send_keys(" pycon")
elem.send_keys(Keys.RETURN)

print(driver.page_source)
driver.quit()
```
#### 通过 Selenium 与页面交互
```
<input type="text" name="passwd" id="passwd-id" />

element = driver.find_element_by_id("passwd-id")
element = driver.find_element_by_name("passwd")
element = driver.find_elements_by_tag_name("input")
element = driver.find_element_by_xpath("//input[@id='passwd-id']") # 多个匹配，只返回第一个；如果没有抛出 NoSuchElementException 异常

element.send_keys("some text")                  # 向文本输入内容
element.send_keys("and some", Keys.ARROW_DOWN)  # 可以对任何获取到到元素使用 send_keys 方法，模拟摸个按键 
element.clear()  # 清除文本

Python

element = driver.find_element_by_xpath("//select[@name='name']")
all_options = element.find_elements_by_tag_name("option")
for option in all_options:
    print("Value is: %s" % option.get_attribute("value"))
    option.click()
```
### 完成表单填充
```
element = driver.find_element_by_xpath("//select[@name='name']")  # 方法一
all_options = element.find_elements_by_tag_name("option")
for option in all_options:
    print("Value is: %s" % option.get_attribute("value"))
    option.click()
    
from selenium.webdriver.support.ui import Select
select = Select(driver.find_element_by_name('name'))  # 方法二
select.select_by_index(index)
select.select_by_visible_text("text")
select.select_by_value(value)


select = Select(driver.find_element_by_id('id'))  # 全部取消
select.deselect_all()

select = Select(driver.find_element_by_xpath("xpath"))  # 获取全部已选项
all_selected_options = select.all_selected_options

options = select.options  # 获取全部可选项

driver.find_element_by_id("submit").click()  # 提交表单
element.submit()  # 单独提交某个元素，元素并没有被表单所包围，那么程序会抛出 NoSuchElementException 的异常
```
### 元素拖拽
```
element = driver.find_element_by_name("source") # 首先指定被拖动的元素和拖动的目标元素
target = driver.find_element_by_name("target")
 
from selenium.webdriver import ActionChains # 利用ActonChains类实现，实现元素source到target的操作
action_chains = ActionChains(driver)
action_chains.drag_and_drop(element, target).perform()
```
### 页面切换
```
driver.switch_to_window("windowName")

for handle in driver.window_handles:
    driver.switch_to_window(handle)

driver.switch_to_frame("frameName.0.child")  # 这样焦点会切换到一个 name 为 child 的 frame 上
```
### 页面等待，隐式等待和显示等待
```
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
 
driver = webdriver.Chrome()                  # 显式等待指定某个条件，设置最长等待时间,如果在这个时间还没有找到元素，那么便会抛出异常 ElementNotVisibleException
driver.get("http://somedomain/url_that_delays_loading")
try:
    element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "myDynamicElement"))
    )
finally:
    driver.quit()
    
wait = WebDriverWait(driver, 10)              # 使用内置的等待条件
element = wait.until(EC.element_to_be_clickable((By.ID,'someid')))

driver = webdriver.Chrome()                   # 隐式等待比较简单，就是简单地设置一个等待时间，单位为秒
driver.implicitly_wait(10) # seconds
driver.get("http://somedomain/url_that_delays_loading")
myDynamicElement = driver.find_element_by_id("myDynamicElement")
```
### 其他
```
alert = driver.switch_to_alert()  # 弹出处理，获取弹窗对象
driver.forward()   driver.back()  # 页面的前进和后退
cookie = {‘name’ : ‘foo’, ‘value’ : ‘bar’}
driver.add_cookie(cookie)         # cookies处理
driver.get_cookies()              # 获得页面的cookies
driver.find_element(By.XPATH, '//button[text()="Some text"]')    # 通过By类确定方式，有ID、XPATH、LINK_TEXT、NAME等 
driver.find_elements(By.XPATH, '//button')
```
#### 执行脚本 driver.execute_script("document.getElementsByClassName('comments dno')[0].click()")

# 13 Selenium 与 requests.Session 对比(http://news.sohu.com/scroll/) 
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/9424BFE29D284A92833F90CE40D57940/2627)
#### 总结：
- 有网友说是Selenium取到了js变量的值，最后得到的是一个 字典；但是如果用常规的requests方式理解，为进行了两次get，最后得到的是一个str，并且尝试了以上的办法也没有实现str到dict的转换。
- 这个例子中就显示了Selenium的优势，得到字典对后去的键值操作非常方便，但是一个str则用处不大；转换过程遇到属性不是双引号包围，尝试category属性两边加上双引号，但是后面的1,2，3等数字又没办法全部加上双引号；对于单引号需要转双引号的问题这里没有。

# 14 http.cookiejar有关cookie的使用
```python
import requests
from http import cookiejar

session = requests.Session()
session.cookies = cookiejar.LWPCookieJar(filename='cookies.txt')
try:
    session.cookies.load(ignore_discard=True)
except LoadError:
    print("load cookies failed")
···
session.cookies.save()
```
# 14 反爬虫[ctrip](https://v.qq.com/x/page/j0308hykvot.html)
- 更改链接地址(price -> prica)
- 更改key， 更改动态key，十分复杂的key(js生成，调试复杂)
- 浏览器检测（检测ie的bug，问ie1+1=3；检测ff的严格性；检测chrome的强大特性
- 抓到了怎么办：给20%假数据；技术压制：前期不管后期直接击杀；放水
```
- 轻松切换目录 cd - , 内部使用$OLDPWD存储前一个cd操作的目录 
- tmux is more powerful shell
```


# 15 TensorFlow  （人工智能-> 深度学习)
![](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/D00C158F5F0E434A8F56F1351FD1D4EC/2999)
[在线动画](http://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.3&regularizationRate=0&noise=0&networkShape=6,5,2,2,2,2&seed=0.68808&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)[github](https://github.com/tensorflow/tensorflow)[极客学院中文文档](http://wiki.jikexueyuan.com/project/tensorflow-zh/)[神经网络与深度学习](https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/)[MNIST Database](http://yann.lecun.com/exdb/mnist/)

```
pip install tensorflow-1.0.1-cp36-cp336m-win_amd64.whl
# initialize_all_variables已被弃用，将在2017-03-02之后删除。说明更新：使用tf.global_variables_initializer代替。

import tensorflow as tf
import numpy as np

# 使用 NumPy 生成假数据(phony data), 总共 100 个点.
x_data = np.float32(np.random.rand(2, 100)) # 随机输入
y_data = np.dot([0.100, 0.200], x_data) + 0.300

# 构造一个线性模型
init = tf.global_variables_initializer()
b = tf.Variable(tf.zeros([1]))
W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))
y = tf.matmul(W, x_data) + b

# 最小化方差
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

# 初始化变量
init = tf.initialize_all_variables()

# 启动图 (graph)
sess = tf.Session()
sess.run(init)

# 拟合平面
for step in range(0, 201):
    sess.run(train)
    if step % 20 == 0:
        print (step, sess.run(W), sess.run(b))

# 得到最佳拟合结果 W: [[0.100  0.200]], b: [0.300]
```
# 15 添加jupyter notebook的配置文件[github extensions](https://github.com/ipython-contrib/jupyter_contrib_nbextensions)[The cell magics in IPython](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb#The-cell-magics-in-IPython)[jupyter-vim-binding](https://github.com/lambdalisue/jupyter-vim-binding)[其他](http://www.jianshu.com/p/3862790b2a50)
#### 要在jupyter notebook中使用多个解释器，Install the IPython kernel[官方](https://github.com/ipython/ipython/blob/7c12b021ee7bdcaf8cec814a624203d8e74aab08/docs/source/install/kernel_install.rst#kernels-for-different-environments)
```
ipython2 or ipython3  # 进入不同版本的ipython

python -m ipykernel install --name scrapy_3env [--display-name 'env_displayed']  # 本人采用

jupyter kernelspec install /tmp/share/jupyter/kernels/python3  # 另一种方式

python2 -m pip install ipykernel
python2 -m ipykernel install --user
```
#### 删除或更新可用的ipython/jupyter内核
```
jupyter-kernelspec list  # 控制台中列出当前的所有内核
rm -rf 'dir'  # 移除对应文件实现删除
```

```
jupyter notebook --generate-config  >>> 生成配置文件 Writing default config to: C:\Users\qiyuan\.jupyter\jupyter_notebook_config.py

c.NotebookApp.notebook_dir = 'D:\Code'  >>> 更改启动时的工作目录

import caffe  # 将本地的.py文件load到jupyter的一个cell中
solvername = '/root/workspace'
sovler = caffe.AdamSolver(solvername)
%load test.py  #test.py是当前路径下的一个python文件;在cell中输入%load http://.....，然后运行该cell，就会将load后面所对应地址的代码load到当前的cell中
```
远程访问jupyter notebook
- 生成秘钥: 打开ipython， from notebook.auth import passwd 然后 passwd()
- 修改配置文件: 
```
c.NotebookApp.ip='*'
c.NotebookApp.password = u'sha:ce...刚才复制的那个密文'
c.NotebookApp.open_browser = False
c.NotebookApp.port =8888 #随便指定一个端口
```
- 启动jupyter notebook，远程访问http://address_of_remote:8888;如果登陆失败，则有可能是服务器防火墙设置的问题，此时最简单的方法是在本地建立一个ssh通道： 
在本地终端中输入ssh username@address_of_remote -L127.0.0.1:1234:127.0.0.1:8888 
便可以在localhost:1234直接访问远程的jupyter了

# 16 Scrapy 学习-1

- scrapy startproject tutorial                 # 管理员+虚拟环境
- scrapy genspider quotes quotes.toscrape.com  # 
- scrapy crawl quotes                          # 运行
```
import scrapy
class QuotesSpider(scrapy.Spider):
    name = 'quotes'

    def start_requests(self):  # 方法一    
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
    
    start_urls = [  # 方法二
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]

    def parse(self, response):
        page = response.url.split('/')[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```
#### 一种方式是定义start_requests方法，由URLs产生scrapy.Request对象；也可以直接通过start_urls类属性，一个关于的urls的列表，默认的实现了start_requests()的功能初始化爬虫的requests。
#### parse()方法处理urls的每次请求，不必明确告知，因为parse()是Scrapy的默认回调函数
- scrapy shell "http://quotes.toscrape.com/page/1/"  # 通过shell提取数据，但当urls包含&符号则不可以， windows中必须使用双引号， 其它时候使用单引号包围
```
response.css('title').extract()  >>> ['<title>Quotes to Scrape</title>']
response.css('title::text').extract()  >>> ['Quotes to Scrape'], ::text表示<title>元素内部的文本元素， .extract()表示提取的是一个SelectorList实例，当然也有.extract_first(), [0].extract(), 使用.extract_first()的好处是当没有找到时返回None，而避免 了IndexError的异常
response.css("div.quote") or quote = response.css("div.quote")[0]

response.css('title').re(r'Quotes.*')  >>> ['Quotes to Scrape'], 也是以使用正则
response.css('title').re(r'Q\w+')  >>> ['Quotes']
response.css('title').re(r'(\w+) to (\w+)') >>> ['Quotes', 'Scrape']

view(response)  # 将打开一个浏览器，可以通过开发者选项提取

response.xpath('//title')  >>> [<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>], 通过xpath指定， css选择器也是转换为了xpath
response.xpath('//title/text()').extract_first()  >>> 'Quotes to Scrape'

tags = quote.css("div.tags a.tag::text").extract()
tags >>>  ['change', 'deep-thoughts', 'thinking', 'world']
response.css('li.next a::attr(href)').extract_first()  >>> '/page/2/'
```
#### 返回数据，提取逻辑，yield返回包含数据项的字典
```
    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').extract_first(),
                'author': quote.css('small.author::text').extract_first(),
                'tags': quote.css('div.tags a.tag::text').extract(),
            }
            
        next_page = response.css('li.next a::attr(href)').extract_first()  # 加入下一页
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
```
#### 指定存储文件: scrapy crawl quotes -o quotes.json
```
scrapy crawl quotes -o quotes-humor.json -a tag=humor # 多了-a参数tag的传递， 将会传递到爬虫的__init__方法中，并且变为爬虫的一个默认属性， 通过self.tag取得
    def start_requests(self):
        url = 'http://quotes.toscrape.com/'
        tag = getattr(self, 'tag', None)
        if tag is not None:
            url = url + 'tag/' + tag  # 将只访问http://quotes.toscrape.com/tag/humor
        yield scrapy.Request(url, self.parse)
        
```

159000&sell_run=0&searchapi_version=eb_split'
#### 下面的例子，由主页开始，找到author页，并之后每一个又调用parse_author回调，分页链接使用的还是原来的parse；默认scrapy通过setting中的DUPEFILTER_CLASS对URLs实现去重
#### 不用extrract()，那么Select对象，还能继续css或xpath；否则是列表，而extract_first()之后就是字符串
```
class AuthorSpider(scrapy.Spider):
    name = 'author'

    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        # follow links to author pages
        for href in response.css('.author + a::attr(href)').extract():
            yield scrapy.Request(response.urljoin(href),
                                 callback=self.parse_author)

        # follow pagination links
        next_page = response.css('li.next a::attr(href)').extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).extract_first().strip()

        yield {
            'name': extract_with_css('h3.author-title::text'),
            'birthdate': extract_with_css('.author-born-date::text'),
            'bio': extract_with_css('.author-description::text'),
        }
```

# 17 Scrapy 学习-2
```
- 全局命令：startproject, genspider, settings, runspider, shell, fetch, view, version
- project-only命令: crawl, check, list, edit, parse, bench
- scrapy startproject <project_name> [project_dir]  #  如果没有指定project_idr，那么将与myproject相同
- scrapy genspider [-t template] <spider_name> <domain>  # <domain>用于产生爬虫的allowed_domains和start_urls属性,scrapy genspider -l用于查看也用的模板，有basic,crawl,csvfeed,xmlfeed;也可以直接建立爬虫源文件也是一样的
- scrapy crawl <spider_name>        # 启动开始一个爬虫
- scrapy check [-l] <spider>        # 进行宏观检查
- scrapy list                       # 列出当前工程下的所有爬虫
- scrapy edit <spider>              # 提供了设置EDITOR得方式，更好地可以在IDE中完成
- scrapy fetch <url>                # 使用scrapy下载和将内容写入输出，可以观察scrapy对指定url的动作，例如查看us是否更改；支持增加其他的参数:--spider=spider_name指定爬虫, --headers打印，--no-redirect，--nolog
- scrapy view <url>                 # 在浏览器中打开指定的url检查，有可能爬虫看到的和用户不同，可以指定--spider=spider_name, --no-redirect
- scrapy shell [url]                # 在控制台中打开指定url，可以指定--spider=spider_name, -c code , --no-redirect, --nolog
- scrapy parse <url> [options]      # 获取并解析，使用--callback或-c指定response的解析回调函数，--spider=spider_name, --a NAME=VALUE, --pipelines, --rules或-r, --noitems, --nolinks, --nocolour, --depth或-d指定深度,--verbose或-v
- scrapy settings [options]         # 如scrapy settings --get BOT_NAME 或 scrapy settings --get DOWNLOAD_DELAY
- scrapy runspider <spider_file.py>  # 只是去运行一个py爬虫文件，并且之前没有创建工程
- scrapy version [-v]
- scrapy benchscrapy bench          # new in 0.17, 快速地benchmark测试
``` 
# 18 Scrapy 学习-3
#### Spiders类

- 首先执行start_requests()方法中的start_urls产生Request对象，并调用回调方法parse();　
- 回调函数中完成response的解析，返回提取的数据，可以是字典、Item对象、Request对象或可迭代对象，他们也将包含回调函数（可以与上面的相同），之后又scrapy自动下载并由回调函数完成相应的处理；
- 在回调函数中，完成内容的解析，通常使用Selectors，当然也可使用擅长的任意解析方式，生成items；
- 最后，返回的items将持久化到数据库，在[Item Pipleine](https://doc.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline)或[Feed exports](https://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports)中指定。

#### scrapy.Spider
最简单的爬虫基类，其他的爬虫都必须进行继承，只提供默认的start_requests()方法，通过start_urls属性发送requests，之后parse()处理请求的responses
```
- name             # 爬虫的名字，必须是唯一的；py2中还必须是ASCII的
- allowed_domains  # 一个列表，规定了爬虫的范围；与开启OffsiteMiddleware意思一样
- start_urls       # 列表
- custom_settings  #  字典，将覆盖工程级的设定，必须定义为类的数学，因为它在实例前执行，[详细](https://doc.scrapy.org/en/latest/topics/settings.html#topics-settings-ref)
- logger           # python logger created with the spider's name, [详细](https://doc.scrapy.org/en/latest/topics/logging.html#topics-logging-from-spiders)
- crawler          # 通过from_crawler()类方法设置，非常强大，[详细](https://doc.scrapy.org/en/latest/topics/api.html#topics-api-crawler)
- settings         # configuration for running this spider,[详细](https://doc.scrapy.org/en/latest/topics/spiders.html)
- 各种法法：from_crawler(crawler, *args, **kwargs), start_requests(), make_requests_from_url(url), parse(response), log(message[,level,component]), closed(reason), [详细](https://doc.scrapy.org/en/latest/topics/spiders.html)

twisted.internet.error.DNSLookupError: DNS lookup failed: address "'http" not found: [Errno 11004] getaddrinfo failed.
>> 将单引号改正为双引号
```
#### 发送POST请求
```
    from scrapy.http import FormRequest
    
    # Start on the welcome page
    def start_requests(self):
        return [
            Request(
                "http://web:9312/dynamic/nonce", callback=self.parse_welcome)
        ]
    # Post welcome page's first form with the given user/pass
        def parse_welcome(self, response):
            return FormRequest.from_response(
                response,
                formdata={"user": "user", "pass": "pass"}
            )
```
#### meta是一个dict，主要是用解析函数之间传递值；多个解析函数直接的传递需要一直带着
一种常见的情况：在parse中给item某些字段提取了值，但是另外一些值需要在parse_item中提取，这时候需要将parse中的item传到parse_item方法中处理，显然无法直接给parse_item设置而外参数。 Request对象接受一个meta参数，一个字典对象，同时Response对象有一个meta属性可以取到相应request传过来的meta。
```
        yield scrapy.Request(url=response.urljoin(category_small_url),
                             callback=self.third_parse,
                             meta={"category_small_name": category_small_name,
                                   "category_big_name": response.meta['category_big_name']})
                                   
        # response.meta.get('key', 'if_not_so_set_the_giving_value')
```


# 19 MongoDB Scrapy Pycharm
```
windows 端口管理windows 端口管理

netstat -a -n  # 以数字形式显示TCP和UDP连接的端口号和状态
netstat -a -n | findstr 27017
cmd >>>  gpedit.msc
```
#### 涉及到考虑端口，是由于存在windows下处理了27017端口占用的情况，采用另指定端口的方式
### 重要
```
mongod --dbpath data\db --port 27018  # 首先：启动服务并指定端口
mongo --port 27018                    # 让后：控制台进入mongo是指定端口，端口不一致看不到show dbs;
# scrapy crawl dangdang  # 启动scrapy爬虫，采用mongo存储;virtualenv用的scrapy_3env
```
scrapy中采用mongodb存储数据，setting.py <-> pipline.py
查看数据采用的是pycharm的mongo插件，进行可视化查看
```
MONGODB_HOST = '127.0.0.1'                  # settings.py
MONGODB_PORT = 27018
MONGODB_DBNAME = "dangdang"
MONGODB_DOCNAME  = "saveinto_2"

import pymongo                                     # piplines.py
from scrapy.conf import settings
from .items import DangdangItem

class DangdangPipeline(object):

    def __init__(self):

        client = pymongo.MongoClient(host=settings['MONGODB_HOST'],
                                     port=settings['MONGODB_PORT'])
        tdb = client[settings['MONGODB_DBNAME']]
        
        self.post = tdb[settings['MONGODB_DOCNAME']]

    def process_item(self, item, spider):
        if isinstance(item, DangdangItem):
            try:
                self.post.insert(dict(item))
            except Exception as e:
                pass
        return item
```
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/D272A088A07248EDB0893F366766ACEE/3481)
        ipython
        ``` 
        import pymongo
        conn = pymongo.MongoClient('localhost', 27018)
        db = conn.dangdang
        coll = db.saveinto_2
        coll.count()      # 暂时是87396条数据
        ```
        cmd
        ```
        mongo --port 27018
        > show dbs;           # admin、dangdang、local; use deomdb 将自动创建一个
        > use dangdang;       
        > show collections;        # 显示doc， svaeinto_1、 saveinto_2
        > db.getCollectionNames()  # 显示doc
        > db.saveinto_2.find()     # 回显记录
        > db.saveinto_1.findOne()  # 回显一条记录
        > db.saveinto_2.count()    # 记录总数
        > db.saveinto_2.count({'book_author': 'Alon'})
        > db.saveinto_2.find({'book_author': 'Alon'}).count()
        
        > db.saveinto_2.insert({'book_author': 'Jack'})
        # 查询条件 $lt $lte $gt $gte 分别对应 < <= > >=   $ne 不等于, 还有 $in $not $or  
        # db.saveinto_2.find({age: {$gt: 33, $lt: 43}})
        
        # null可以匹配自身, 而且可以匹配"不存在的"
        > db.Student.insert({name:null,sex:1,age:18}) # 插入一条测试数据
        > db.Student.insert({sex:1,age:24})
        > db.Student.find({name:null})        --上面两条都能查到
        > db.Student.find({name:{$in:[null],$exists:true}})  ---只能查到第一条 
        
        > db.Student.find().sort({age:1,sex:1})  # 指定排序， 1升序 -1降序
        > db.Student.find().sort({age:1}).limit(3).skip(3)  # 限制
        > db.Student.remove({name:null})             # 删除数据
        > db.Student.update({name:"jack"},{age:55})  # 更新数据
        > db.Student.update({name:"lily"},{$inc:{height:175}})   # 增加指定量
        ```

### 重点补充：
- scrapy 中如果含有多级的解析，如 parse -> parse_2 -> parse_3 , 其中如果传递参数，建议使用item方式
- 如果不是一开始就初始一个 DangdangItem() , 而每次都带着自己构造的字典进行参数传递， 可能在多个parse之后出现 KeyError 错误，提示找不到自己字典的键值
- 学习中发现，有时中断scrapy的过程后，在此开始但是一条结果都找不到，返回finished；通过在每一次scrapy.Respect() 中增加参数dont_filter=True, 表示不筛选，否则筛选url
- 
```
    def parse(self, response):
        item = DangdangItem()
        item['category_big_name'] = goods.xpath('a/@title')[0] 
        return yield scrapy.Request(url= , cookises=, headers=, meta={'item': item}, callback=self.parse_2, dont_filter=True)

    def parse_2(self, response):
        item = response.meta['item']
        item['category_small_name'] = goods.xpath('a/@title')[0]
        return yield scrapy.Request(url=response.urljoin('**'), callback=self.third_parse, meta={'item': item}, dont_filter=True)

    def parse_3(self, response):
        """同理"""
        return item
        """此处还可以继续翻页"""
```

# 20 定时任务设置crontab 和 python-crontab
#### Linux 自带定时任务功能 crontab　
crontab [-u user] -l 查看当前用户已有定时任务
crontab [-u user] -e 编辑当前用户定时任务
crontab [-u user] -r 删除当前用户所有定时任务
crontab [-u user] -v 查看当前用户所有定时任务状态

- 首先编辑一个crontab文件，使用规定的格式，[在线生成](https://crontab.guru/every-1-minute)时间字段
- 之后提交crontab文件 crontab cronfile 这样就将crontab文件提交给cron进程，并保存副本到/var/spool/cron/crontabs/用户名 
### 实战：定时每一分钟执行一次scrapy下的项目
```
* * * * * cd /home/ubuntu/qiyuan && /usr/local/bin/scrapy crawl quotes
```
- 文件:   /etc/cron.deny  该文件中所列的用户不允许使用crontab命令， 对应的是.allow
- 文件:   /var/spool/cron/  所有用户crontab文件存放的目录,以用户名命名;每一行都代表一项任务,格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段

#### pip install python-crontab  
提供python脚本进行自动化执行管理，[link](https://pypi.python.org/pypi/python-crontab)
##### chmod 控制文件或目录的访问权限
- 表示增加权限、- 表示取消权限、= 表示唯一设定权限, 如chmod +x
-  Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他
-  u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是
- r 表示可读取，w 表示可写入，x 表示可执行，X表示只有当该档案是个子目录或者该档案已经被设定过为可执行
- 此外chmod也可以用数字来表示权限如 chmod 777 file 


# 21 Pycharm mongo plugin 搜索功能
之前使用插件实现了在pycharm中查看mongo数据库，但是最多可以显示299条数据; 可以实现如下的功能，能多参考[github  link](https://github.com/dboissier/mongo4idea)
- filter：接受一个字典作为查询条件。查询空值字段，指定值为null，而不是python的None
- projection：字段选择。接受一个字典作为字段选择，如果字段值为1，显示，字段值为0，不显示，一般字段默认不显示，'_id'字段默认显示，如果选择不显示'_id'字段，必须放在字典的第一个键值对位置
- sort：接受一个字典作为排序条件，字段值为1，按从上到下升序排列，字段值为-1，按从上到下降序排列

![](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/2DEB60E893C24D4A80A654E8105720B3/3680)



# 22 如何调试 debug scrapy
需要新建文件，命名任意，但是要写入下面的代码，之后右键 debug 此文件即可
```
from scrapy import cmdline
cmdline.execute("scrapy crawl your_spider_name".split())
```
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/8EEE062648954BB9B79446A6336329C5/3711)

# 23 通过 map 函数完成并行
#### 有两个库： multiprocessing, multiprocessing.dummy; dummy就是多进程模块的克隆文件，唯一不同的是多进程模块使用的是进程，而 dummy 使用的是线程。
``` python3
    # from multiprocessing import Pool
    import urllib
    from multiprocessing.dummy import Pool as ThreadPool
    
    urls = ['url1', 'url2', 'url3', 'url4', 'url5']
    pool = ThreadPool(3)  # 完成初始化， 默认为电脑内核数量
    rets = pool.map(urllib.urlopen, urls)
    pool.close()
    pool.join()  # 关闭 pool 并等待 work 完成
```
#### 生成缩略图(美观 + 易调试)
```
    import os, PIL
    from multiprocessing import Pool
    from PIL import Image
    
    SIZE = (75, 75)
    SAVE_DIRECTORY = 'thumbs'
    
    def get_image_paths(folder):
        return (os.path.join(folder, f) 
                for f in os.listdir(folder) if 'jpeg' in f)
    
    def create_thumbnail(filename):
        img = Image.open(filename)
        im.thumbnail(SIZE, Image, ANTIALIAS)
        base, fname = os.path.split(filename)
        save_path = os.path.join(base, SAVE_DIRECTORY, fname)
        im.save(save_path)
        
    if __name__ == '__main__':
        folder = os.path.abspath('***')
        os.mkdir(os.path.join(folder, SAVE_DIRECTORY))
        imgs = get_image_paths(folder)
        
        pool = Pool()
        pool.map(create_thumbnail, imgs)
        pool.close()
        pool.join()
        
```
# 24 使用localhost:4000访问本地blog一直无响应 
- Hexo 3.0 把服务器独立成了个别模块，您必须先安装 hexo-server 才能使用。

  ```$ npm install hexo-server --save```
- 安装完成后，输入以下命令以启动服务器，您的网站会在 http://localhost:4000 下启动。在服务器启动期间，Hexo 会监视文件变动并自动更新，您无须重启服务器。

   ```$ hexo server```
- 如果您想要更改端口，或是在执行时遇到了 EADDRINUSE 错误，可以在执行时使用 -p 选项指定其他端口，如下：

   ```$ hexo server -p 5000```
# ·hexo· (A fast, simple & powerful blog framework)
```
$npm install hexo-cli -g # 在之前需要安装nodejs
$hexo init blog
$cd blog
$npm install
$hexo server
```
# 搭建 hexo，在执行 hexo deploy 后,出现 error deployer not found:git 的错误
```npm install hexo-deployer-git --save``` 可以解决
```
hexo clean 
hexo generate
hexo deploy  # 最后，发布上去
```

```
git init # 有时候，部署完发现没有更新内容，实际就是部署失败，如下图，但git init后可以解决
```
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/25266E38AE734BD2A5DF62E6BAB5EF8B/1839)


##  [git] warning: LF will be replaced by CRLF 或 fatal: CRLF would be replaced by LF 
Git提供了一个换行符检查功能（core.safecrlf），可以在提交时检查文件是否混用了不同风格的换行符。这个功能的选项如下：
- false - 不做任何检查
- warn - 在提交时检查并警告
- true - 在提交时检查，如果发现混用则拒绝提交，建议使用最严格的 true 选项。

假如你正在Windows上写程序，又或者你正在和其他人合作，他们在Windows上编程，而你却在其他系统上，在这些情况下，你可能会遇到行尾结束符问题。这是因为Windows使用回车和换行两个字符来结束一行，而Mac和Linux只使用换行一个字符。虽然这是小问题，但它会极大地扰乱跨平台协作。

Git可以在你提交时自动地把行结束符CRLF转换成LF，而在签出代码时把LF转换成CRLF。用core.autocrlf来打开此项功能，如果是在Windows系统上，把它设置成true，这样当签出代码时，LF会被转换成CRLF：

```
git config --global core.autocrlf true
```
Linux或Mac系统使用LF作为行结束符，因此你不想 Git 在签出文件时进行自动的转换；当一个以CRLF为行结束符的文件不小心被引入时你肯定想进行修正，把core.autocrlf设置成input来告诉 Git 在提交时把CRLF转换成LF，签出时不转换：

```
git config --global core.autocrlf input
```
这样会在Windows系统上的签出文件中保留CRLF，会在Mac和Linux系统上，包括仓库中保留LF。

如果你是Windows程序员，且正在开发仅运行在Windows上的项目，可以设置false取消此功能，把回车符记录在库中：
```
git config --global core.autocrlf false
```
Error: Could not open a connection to your authentication agent
```
eval $(ssh-agent)  # run it befor ssh-add

```
# 25 一定要以管理员权限进行hexo g -d
```
Error: fatal: AggregateException encountered.
   One or more errors occurred.
bash: /dev/tty: No such device or address
error: failed to execute prompt script (exit code 1)
fatal: could not read Username for 'https://github.com': No error

    at ChildProcess.<anonymous> (D:\Code\blog\node_modules\hexo-util\lib\spawn.js:37:17)

    at emitTwo (events.js:106:13)
    at ChildProcess.emit (events.js:194:7)
    at ChildProcess.cp.emit (D:\Code\blog\node_modules\cross-spawn\lib\enoent.js:40:29)
    at maybeClose (internal/child_process.js:899:16)
    at Socket.<anonymous> (internal/child_process.js:342:11)
    at emitOne (events.js:96:13)
    at Socket.emit (events.js:191:7)
    at Pipe._handle.close [as _onclose] (net.js:513:12)
```
### hexo g -d 其实这个错误与网络也有关系，有关系
github的主页分为项目主页和个人主页，其中项目主页的介绍[查看](https://pages.github.com/)。
- http://username.github.io/repository
- http://username.github.io

#### 之后可以词用麦田音乐网的评论系统，挺喜欢
有一个好处是可以随时评论，评完即走，类似于微信小程序。应该是用的WordPress搭建，[源码](https://github.com/WordPress/WordPress/blob/master/wp-comments-post.php)。我还是比较喜欢这样的方式，简洁，相比较引入[多说](http://duoshuo.com/)的评论系统。还有Disqus评论。目前，hexo博客框架的官方next主题已经支持多说，Disqus，Hypercomments，网易云跟贴等多种第三方评议系统，但是官方的文档并没有对这部分的内容及时进行更新，所以广大朋友并不知道除了多说，disqus之外，其实还有很多可以进行选择的。
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/7B3ABCA32D104F71B00D3D200C7A7E44/1859)


#### Adding ClustrMaps to your page footer
站点统计工具，统计ip地址，以图的形式展现[。clustrMaps](http://clustrmaps.com/)

#### 初级阶段hexo的总结
![image](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/4DC9096193DF40A2BD975247BB42E966/1933)
- Git连接ssh： 设置Git的user name和email
```
git config --global user.name "aqiongbei" #改成你的注册Github的用户名
git config --global user.email "aqiongbei@gmail.com" #改成你的注册Github的邮箱
```
- Git连接ssh： 生成SSH密钥
```
ssh-keygen -t rsa -C "aqiongbei@gmail.com" #改成你注册Github的邮箱
```
- 添加密钥到 Github

```
点击自己的头像->settings->SSH Keys->Add SSH key
将本地 id_rsa.pub 中的内容粘贴到 Key 文本框中，随意输入一个 title，点击 Add Key 即可
```
- 测试Git
```
ssh -T git@github.com   # 或使用参数-vT，可以显示过程
```
- 安装hexo: npm install -g hexo-cli
- 新建一个网站: hexo init
- 添加一个文档: hexo new 'name'
- hexo clean | hexo g -d | hexo s -p 4000
- npm install hexo-deployer-git --save
# 5 npm安装hexo速度过慢

由于某些大家都知道的缘故，npm官方源在国内的下载速度极其慢，用官网的npm install hexo-cli -g速度非常感人，所以不推荐这种方式。这里我推荐用淘宝的npm分流——cnpm
安装过程很简单：
npm install -g cnpm --registry=https://registry.npm.taobao.org
然后等着装完即可，之后的用法和npm一样，无非是把npm install改成cnpm install,但是速度比之前快了不止一个数量级(不过下文为了方便理解，还是会用默认的npm安装，如果你发现速度不好的话，请自行替换成'cnpm')

#### 更换主题 [yelee](https://github.com/MOxFIVE/hexo-theme-yelee)
```
---
title: 技术杂项_1_记录(已完成)
date: 2017-03-30
tags:
---
<Excerpt in index | 首页摘要>
实现Hexo首页展示，这里显示固定大小的一块简介>
，但是由于没有什么可以写的，反而排版不好了不>
如就是什么都不想，也不要这个排版，就是让他世>
界显示吧。
<!-- more -->
<The rest of contents | 余下全文>  # 这个尖括号也算作是开始的正文部分
```
关于配置文件的修改之后，需要hexo clean，才能生效。[一次执行多个爬虫](https://doc.scrapy.org/en/latest/topics/practices.html?highlight=scrapy.crawler.CrawlerProcess)

# 26 对比分析
    os.file.dirname(__file__) 
    >> /home/ubuntu/app_1
    返回脚本的路径；不能再命令行执行，否则返回NameError:name '__file__' is not defined；如果运行python /home/ubuntu/app_1/utils.py 返回如上，如果用相对路径运行python utils.py 返回空字符串
    
    os.file.abspath(__file__) 
    >> /home/ubuntu/app_1/utils.py 
    返回.py文件的绝对路径，所以结合起来使用最好
    
    os.path.abspath(os.path.dirname(__file__)
    >> /home/ubuntu/app_1
# 27 偏函数 from functools import partial 局部的、部分的
函数调用的时候有多个参数，但其中一个参数已经知道了，可以通过这个参数重新绑定一个新的函数，之后可以去调用这个新的函数;当函数的参数个数太多，需要简化时，使用functools.partial可以创建一个新的函数，这个新函数可以固定住原函数的部分参数，从而在调用时更简单
```python
    def add(a, b):
        return a + b
    add(4, 3)
    >> 7
    add100 = partial(add, 100) # 将原来的add变为add + 100
    add100(9)
    >> 109
```
扩展的功能，当含有默认参数时
```python
    def add(a, b, c):
        print a, b, c
        return a + b + c
    plus = partial(add, 1, 2)
    plus(5)
    >> 1 2 5
    >> 8 
    def add(a, b, c=2):
        print a, b, c
    plus = partial(add, 101)
    plus(1)
    >> 101 1 2
    plus = partial(add, 101)(1)
    >> 101 1 2
    plus = partial(add, b=101)(1)
    >> 1 101 2 
``` 

#### 并发编程
- theading 

- multiprocessing 多进程模块

- multiprocessing.dummy 多线程模块

- 线程模块同步原语，包括锁（Lock），信号量（Semaphore），条件变量（Condition）和事件（EVent）；最好使用Queue模块，它是线程安全的，使用它可以降低程序的复杂度、代码清晰、可读性强。

- 协成Gevent，上下文切换通过yield完成，执行gevent.sleep会触发上下文切换；协成池from Gevent.pool import Pool

- concurrent.futures中包含ThreadPoolExecutor和ProcessPoolExecutor两个执行器，分别用于产生线程池和进程池
- Python2通过生成器（Generator）实现协成，利用yield返回，send或next发送数据；Python对协成的支持大抵如此，一般使用第三方库实现的协成来编写程序。Python3.3中添加yield from，允许生成器把它的部分操作委任给另一个生成器；Python3.4中asyncio被纳入标准库；Python3.5添加async和await两个关键字，协成成为新的语法，不在是一种生成器了。I/O多路复用与协成的引入，极大地提高了高负载下程序的I/O性能。
async用于声明一个协成；await表示一个协成执行完返回，获得协成执行结果，只能在协成内使用。
async简化了asyncio.coroutine，await简化了yield from

#### aiohttp替代requests
- 可作为HTTP客户端
- 实现HTTP服务
- 
# 5 zip, izip 和 izip_logest比较
- zip是build-in方法，返回长度为序列中最短的,返回的是<zip object at 0x00008*8>对象
- python2：itertools中的izip，将不同的迭代器元素聚合到一个迭代器中，比zip要快得多
- python2：itertools中的izip_longest，使用最长的迭代器作为返回值的长度，使用fillvalues指定缺失值得默认值
- python3：itertools中只保留了zip_longest,亦可指定默认值；
``` python3
from itertools import zip_longest

class Solution(object):
    def addTwoNumbers(self, l1, l2):
        """
        :type l1: ListNode
        :type l2: ListNode
        :rtype: ListNode
        """
        newlist, carry = [], 0
        for n, m in list(itertools.zip_longest(l1, l2, fillvalue=0)):
            if n + m + carry < 10:
                newlist.append(n + m + carry)
                carry = 0
            else:
                newlist.append(n + m + carry - 10)
                carry = 1
        return newlist
        
if __name__ == '__main__:'
    s = Solution()
    s.addTwoNumbers([2,4,3], [5,6,4])
```

# 28 按照指定位置旋转字符串,nums, k
```
def rotate(nums, k):
    k = k % len(nums)
    nums = nums[-k:] + nums[:-k]
    return nums

print(rotate('1234567', 4)
>>> 4567123
```
#### python 小记
- Queue 和 Stack 在 Python 中都是有 list ,[] 实现的。 在python 中list是一个dynamic array, 可以通过append在list的尾部添加元素， 通过pop()在list的尾部弹出元素实现Stack的FILO， 如果是pop(0)则弹出头部的元素实现Queue的FIFO。
- python 中通过import heapq实现优先队列（Priority Queue),提供push和pop两个最基本的操作和heapify初始化操作。
- 双端队列（deque，全名double-ended queue）可以让你在任何一端添加或者移除元素，因此它是一种具有队列和栈性质的数据结构；Python 的list就可以执行类似于deque的操作， 但是效率会过于慢。 为了提升数据的处理效率， 一些高效的数据结构放在了collections中。 在collections 中提供了deque的类， 如果需要多次对list执行头尾元素的操作， 请使用deque。
- 堆Heap， 一般情况下，堆通常指的是二叉堆，二叉堆是一个近似完全二叉树的数据结构，即披着二叉树羊皮的数组，故使用数组来实现较为便利。子结点的键值或索引总是小于（或者大于）它的父节点，且每个节点的左右子树又是一个二叉堆(大根堆或者小根堆)。根节点最大的堆叫做最大堆或大根堆，根节点最小的堆叫做最小堆或小根堆。常被用作实现优先队列。
- 栈Stack， 栈是一种 LIFO(Last In First Out) 的数据结构，常用方法有添加元素，取栈顶元素，弹出栈顶元素，判断栈是否为空。
- 集合Set， 保存不重复元素的数据结构，是python自带的基本数据结构，有多种初始化方式，s = set()
- 图Graph, 表示通常使用邻接矩阵和邻接表，前者易实现但是对于稀疏矩阵会浪费较多空间，后者使用链表的方式存储信息但是对于图搜索时间复杂度较高。设顶点个数为 V, 那么邻接矩阵可以使用 V × V 的二维数组来表示。 g[i][j]表示顶点i和顶点j的关系，对于无向图可以使用0/1表示是否有连接，对于带权图则需要使用INF来区分。有重边时保存边数或者权值最大/小的边即可。
```
V = 5
g = [[0 for _ in range(V)] for _ in range(V)]
print(g)
>>> [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
```
- 二叉树Binary Tree, 二叉树是每个节点最多有两个子树的树结构，子树有左右之分，二叉树常被用于实现二叉查找树和二叉堆。二叉树的第i层至多有 2^{i-1}个结点；深度为k的二叉树至多有 2^k−1个结点；对任何一棵二叉树T，如果其终端结点数为 n0, 度为2的结点数为 n2 , 则 n0=n2 + 1。一棵深度为 k, 且有 2^k −1 个节点称之为满二叉树；深度为 k，有 n个节点的二叉树，当且仅当其每一个节点都与深度为 k 的满二叉树中序号为 1 至 n的节点对应时，称之为完全二叉树。完全二叉树中重在节点标号对应。
- Binary Search Tree , 二叉查找树,一颗二叉查找树(BST)是一颗二叉树，其中每个节点都含有一个可进行比较的键及相应的值，且每个节点的键都大于等于左子树中的任意节点的键，而小于右子树中的任意节点的键。
使用中序遍历可得到有序数组，这是二叉查找树的又一个重要特征。
- Map 哈希表， 是一种关联数组的数据结构，常被称为字典或键值对。在 Python 中 dict(Map) 是一种基本的数据结构。

# 7 python 小记
- python中有三种方法，实例方法、类方法（@classmethod）、静态方法（@staticmethod）
- 类方法的第一个参数时cls，表示该类的一个实例，静态方法与普遍的方法一样
- self和cls的区别不是强制的，只是PEP8中一种编程风格，slef通常用作实例方法的第一参数，cls通常用作类方法的第一参数。即通常用self来传递当前类对象的实例，cls传递当前类对象。
```
class A(object):
    def foo(self, x):
        print("executing foo(%s,%s)" % (self, x))
        print('self:', self)
    @classmethod
    def class_foo(cls, x):
        print("executing class_foo(%s,%s)" % (cls, x))
        print('cls:', cls)
    @staticmethod
    def static_foo(x):
        print("executing static_foo(%s)" % x)    
a = A()

print(a.foo)           >>> <bound method A.foo of <__main__.A object at 0x0000000004F6B908>>
print(a.class_foo)     >>> <bound method A.class_foo of <class '__main__.A'>>
print(a.static_foo)    >>> <function A.static_foo at 0x0000000004F05488>
```
#### foo方法绑定对象A的实例，class_foo方法绑定对象A，static_foo没有参数绑定。

\ | 实例方法 | 类方法 | 静态方法
---|--- | --- | ---
a = A() | a.foo(x) | a.class_foo(x) | a.static_foo(x)
A  | 不可用 | A.class_foo(x) | A.static_foo(x)

#### 实例是三种方法都可以调用的，而类只可以调用两种;
- 类方法（classmethod）必须要用类对象作为第一个参数，静态方法（staticmethod）可以没有参数
- [详细介绍1](http://stackoverflow.com/questions/12179271/meaning-of-classmethod-and-staticmethod-for-beginner),[详细介绍2](http://blog.csdn.net/a447685024/article/details/52424481)
- 类方法的第一个参数cls，而实例方法的第一个参数是self，表示该类的一个实例;类方法有类变量cls传入，从而可以用cls做一些相关的处理。并且有子类继承时，调用该类方法时，传入的类变量cls是子类，而非父类。 

# 8 闭包
- 可以实现将参数传递给函数，但不立即求值，达到延迟求值的目的。
- 闭包满足三个条件：必须有一个内嵌的函数；内嵌的函数必须引用外部函数中的变量；外部函数返回值必须是内嵌函数。
- 类中定义函方法 PyCharm 提示Method xxx may be ‘static’, 原因是该方法不涉及对该类属性的操作，编译器建议声明为@staticmethod，面向对象思想体现。
```
def dalay_func(x, y):
    def caculator():
        return x + y
    return caculator

msum =dalay_func(3, 4)
print(msum())            >>> 7
```
#### *args 和 **kwargs
- 为python中的可变参数，*args表示任意多个无名参数，是一个元祖
- **kwargs表示关键字参数，是一个字典；同时使用时必须*args在前，**kwargs在后

#### @property @x.setter @x.deleter
- 通过@property装饰器，将类方法转换为类属性（只读），可通过正常的点符号访问，但不能赋值（AtributeError）
- 可以有@x.setter, @x.getter, @x.deleter
```
class C(object):
    def __init__(self): self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        return self._x

    @x.setter
    def x(self, value):
        self._x = value

    @x.deleter
    def x(self):
        del self._x
```
#### python 快速排序
```

def qsort(seq):
    if len(seq) <= 1:
        return seq
    else:
        pivot = seq[0]
        lesser = qsort([x for x in seq[1:] if x < pivot])
        greater = qsort([x for x in seq[1:] if x >= pivot])
        return lesser + [pivot] + greater

print(qsort([1, 3, 4, 5, 0, -3, -9, 22])  >>> [-9, -3, 0, 1, 3, 4, 5, 22]
```
#### Arrow: better dates and times for Python  [详细](https://arrow.readthedocs.io/en/latest/)
#### 友好的创建、操作、格式化、转换，日期、时间和时间戳，克服了以下问题：
- 模块太多：datetime, time, calendar, dateutil, pytz and more
- 类型太多： date, time, datetime, tzinfo, timedelta, relativedelta and more
- 时区和时间戳之间的转换冗余、不友好， 幼稚的时区
- 功能缺失：ISO-8601 parsing， time spans, humanization

```
import arrow

arrow.utcnow()           >>> <Arrow [2017-04-03T02:07:01.518490+00:00]>
arrow.now()              >>> <Arrow [2017-04-03T10:07:00.915456+08:00]>
arrow.now('US/Pacific')  >>> <Arrow [2017-04-02T19:07:02.060521-07:00]>

arrow.get(1367900664)    >>> <Arrow [2013-05-07T04:24:24+00:00]>
arrow.get('1367900664')  >>> <Arrow [2013-05-07T04:24:24+00:00]>
arrow.get('1367900664.15232522')  >>> <Arrow [2013-05-07T04:24:24.152325+00:00]>
arrow.get(datetime.datetime.utcnow())  >>> <Arrow [2017-04-03T02:16:38.217476+00:00]>
arrow.get(datetime.datetime.utcnow(), 'US/Pacific')  >>> <Arrow [2017-04-03T02:17:49.488552-07:00]>

arrow.get('2013-05-05 12:30:45', 'YYYY-MM-DD HH:mm:ss') # 由字符串解析
arrow.get('June was born in May 1980', 'MMMM YYYY'      # 由字符串搜索得到
arrow.get('2013-09-30T15:34:00.000-07:00')  # ISO-8601兼容的字符串可以直接识别和解析，不用指定格式字符串 

arrow.get(2013, 5, 5)
arrow.Arrow(2013, 5, 5)

a = arrow.utcnow()  >>> <Arrow [2017-04-03T02:23:16.383090+00:00]>
a.datetime  >>> datetime.datetime(2017, 4, 3, 2, 23, 16, 383090, tzinfo=tzutc())
a.naive  >>> datetime.datetime(2017, 4, 3, 2, 23, 16, 383090)
# a.tzinfo, a.year, a.date(), a.time(), a.replace(hours=-1, minute=40, weeks=+3), a.format('YYYY-MM-DD HH:mm:ss ZZ'), a.to('US/Pacific'), a.to('local'), a.to('local').to('utc')
# b = a.replace; b.humanize() Humanize，会输出一个字符串，‘an hour ago', 'in 2 hours'

arrow.utcnow().span('hour'), arrow.utcnow().floor('hour'), arrow.utcnow().ceil('hour')
start, end = datetime(2017, 1, 1, 1, 1), datetime(2017, 1, 1, 3, 3)
for r in arrow.Arrow.span_range('hour', start, end):
    print(r)  # print(repr(r))
```

# 28 可以查看当下流行的Python库
#### Trending in open source， See what the GitHub community is most excited about today.
[github](https://github.com/trending/python)![](http://note.youdao.com/yws/public/resource/9de657f6beaffa68f2619e3f1fa38098/xmlnote/C364DFBD50104A3B904D72D4A56CE165/2990)


